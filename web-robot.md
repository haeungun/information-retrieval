# Web Robot
웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다. 많은 로봇들이 웹 사이트에서 다른 웹 사이트로 
떠돌아다니면서, 콘텐츠를 가져오고, 하이퍼링크를 따라하고, 그들이 발견한 데이터를 처리한다. 이러한 종류의 로봇들을 그 방식에 따라 
'크롤러', '스파이더', '웜', '봇' 등 다양한 이름으로 불린다. </br>

웹 크롤링은 컴퓨터 소프트웨어 기술로 웹 사이트들에서 원하는 정보를 추출하는 것을 의미한다.
웹 로봇, 웹 크롤러는 인터넷 상의 웹 페이지를 방문해서 자료를 수집하는 일을 하는 프로그램을 말한다.
이때 한 페이지만 방문하는 것이 아니라 그 페이지에 링크되어 있는 또 다른 페이지를 차례대로 방문하고 이처럼 링크를 따라 웹을 돌아다니는 
모습이 마치 거미와 비슷하다고 해서 스파이더라고 부르기도 한다.
> Web Crawling 의 정식명칭은 Web Scraping

웹 환경은 다음과 같은 특성들을 지니고 있으며, 웹 로봇은 이러한 특성들을 고려하여 개발되어야 한다. 

- 웹에 공개되는 문서들의 수가 매우 빠르게 증가하고 있다.
- 웹 로봇이 문서를 수집하고 있는 동안에도 이들에 대한 수정 or 삭제가 수행된다.
- 웹에는 다수의 유해 or 스팸 웹 문서도 존재한다.
- 웹에는 동일한 내용을 지닌 문서들이 존재한다.
- 로봇 배제 표준이 준수되어야 한다.

## 크롤러와 크롤링
웹 크롤러는 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹페이지들을 가져오는 
이러한 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다. 웹 링크를 재귀적으로 따라가는 로봇을 크롤러 혹은 스파이더라고 부른다.</br>
인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다. 이 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어 
지는데, 이는 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다. 찾아서 가져와야 하는 페이지들이 수십억 개나 되다 보니 필연저으로 이들 검색엔진 
스파이더들은 가장 복잡한 로봇들 중 하나가 되었다.

#### 어디에서 시작하는가
크롤러는 우선 출발지점을 지정해주어야 한다. 크롤러가 방문을 시작하는 URL 들의 초기 집합은 `root set` 이라고 불린다. root set 을 고를 때, 모든 링크를 
크롤링하면 결과적으로 관심 있는 웹페이지들의 대부분을 가져오게 될 수 있도록 충분히 다른 장소에서 URL 들을 선택해야 한다. </br>
일반적으로 좋은 root set 은 크고 인기 있는 웹사이트(ex. www.google.com), 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려져 있지 않은 
페이지들의 목록으로 구성되어 있다. 인터넷 검색엔진에서 쓰이는 것과 같은 많은 대규모 크롤러 제품들은 사용자들이 루트 집합에 새 페이지나 잘 알려지지 않은 
페이지들을 추가하는 기능을 제공한다. 이 root set 은 시간이 지남에 따라 성장하며 새로운 크롤링을 위한 시드 목록이 된다.

#### 링크 추출과 상대 링크 정상화
크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색한다. 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가 해야 한다. 
크롤러가 크롤링을 진행하면서 탐색해야 할 새 링크를 발견함에 따라 이 목록은 일반적으로 급속히 확장된다. 
> 순환을 피하기 위해서 크롤러는 그들이 방문한 곳을 기억할 필요가 있다. 크롤링이 진행되면서 웹 공간이 완전히 모두 탐색되어서 더 이상 크롤러가 방문할 새 링크가 없게 되는 시점에 이를 때까지 방문한 URL 의 목록은 계속 성장한다. 


## 평가 기준
**웹 로봇의 성능**을 체계적으로 평가하기 위한 기준으로서 효율성, 지속성, 신선성, 포괄성, 정숙성, 유일성, 안정성을 제시하고, 
이러한 평가 기준들의 향상에 도움이 되는 기능들이 있다. 
웹 로봇 개발자는 각각의 평가 기준에 대하여 열거된 기능들을 웹 로봇에 추가함으로써, 해당 평가 기준을 향상시키고, 보다 높은 성능의 
웹 로봇을 개발할 수 있다. 

### 효율성
주어진 시간 내에 얼마나 많은 웹 문서들을 수집할 수 있는가.

- 병렬수집
````
웹 로봇은 수집된 웹 문서들을 디스크에 저장하기 위해 많은 시간을 소비하는 I/O intensive program 이다. 
따라서 하나의 컴퓨터에서 다수의 웹 로봇 프로세스를 동시에 실행함으로써 주어진 시간에 보다 많은 웹 문서를 수집할 수 있다.
````
- 분산수집
````
다수의 컴퓨터를 이용한 웹 문서들의 분산 수집은 웹 문서 수집에 소비되는 시간을 단축시킬 수 있다.
````
- 분산저장
````
일반적으로 웹 로봇 시스템은 서로 다른 기능을 수행하는 다수의 컴퓨터로 구성되며, 네트워크를 통하여 웹 문서들을 내려 받는 기능과 웹 문서들을 
저장하는 기능은 별도의 장비에서 수행된다.
따라서 하나의 저장 서버에 모든 웹 문서들을 저장할 경우, 저장 서버에 입출력 병목 현상이 발생할 수 있기 때문에, 
웹 문서들을 다수의 저장 서버에 분산 저장하는 것이 바람직하다.
````
- DNS 캐시
````
하나의 컴퓨터에서 다수의 웹 로봇 프로세스들을 실행하고, 다수의 컴퓨터를 이용하여 웹 문서들을 수집할 경우 URL 을 IP 주소로 변환하는 
DNS 서버에 병목 현상이 발생할 수 있다. 이러한 병목 현상은 호스트 이름과 IP 주소를 자체적으로 관리함으로써 줄일 수 있다.
````

### 지속성
일반적으로 웹 검색 서비스들은 주기적으로 웹 로봇을 수행.</br>
지속성은 이전에 수집된 후 현재까지 삭제되지 않은 웹 문서들에 대한 재수집된 웹 문서의 비율로 정의된다.</br>
즉, 지속성은 이전에 수집된 후 현재까지 삭제되지 않은 웹 문서들이 어느 정도 재수집 되었는가를 나타낸다. 

### 신선성
### 포괄성
### 정숙성
### 유일성
### 안정성
